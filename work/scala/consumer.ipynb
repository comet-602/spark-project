{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>23</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "<console>:26: error: object kafka09 is not a member of package org.apache.spark.streaming\n",
      "       import org.apache.spark.streaming.kafka09.{ConsumerStrategies, KafkaUtils, LocationStrategies}\n",
      "                                         ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.kafka.clients.consumer.ConsumerConfig\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import org.apache.spark.streaming.kafka09.{ConsumerStrategies, KafkaUtils, LocationStrategies}    \n",
    "\n",
    "val ssc = new StreamingContext(sc, Seconds(1))\n",
    "\n",
    "val topicsSet = Set(\"/streaming_test/test_stream:test_topic\")\n",
    "val kafkaParams = Map[String, String](\n",
    "  ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> \"10.120.26.21:9092\",\n",
    "  ConsumerConfig.GROUP_ID_CONFIG -> \"none\",\n",
    "  ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG ->\n",
    "    \"org.apache.kafka.common.serialization.StringDeserializer\",\n",
    "  ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG ->\n",
    "    \"org.apache.kafka.common.serialization.StringDeserializer\",\n",
    "  ConsumerConfig.AUTO_OFFSET_RESET_CONFIG -> \"earliest\",\n",
    "  ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG -> \"false\" \n",
    ")\n",
    "\n",
    "val consumerStrategy =\n",
    "      ConsumerStrategies.Subscribe[String, String](topicsSet, kafkaParams)\n",
    "val messages = KafkaUtils.createDirectStream[String, String](\n",
    "      ssc,\n",
    "      LocationStrategies.PreferConsistent,\n",
    "      consumerStrategy)\n",
    "\n",
    "val lines = messages.map(_.value())\n",
    "val words = lines.flatMap(_.split(\" \"))\n",
    "val wordCounts = words.map(x => (x, 1L)).reduceByKey(_ + _)\n",
    "wordCounts.print()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTerminationOrTimeout(3 * 60 * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
