{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.stop()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "conf = SparkConf().setAppName(\"zzzZ\")\\\n",
    "                  .setMaster(\"spark://10.120.26.200:7077\")\\\n",
    "                  .set(\"spark.executor.memory\", '6g')\\\n",
    "                  .set('spark.cores.max', '4')\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.appName('sql coming~').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+-----+\n",
      "| id|name|age| text|\n",
      "+---+----+---+-----+\n",
      "|  1|Suny| 10|water|\n",
      "|  2|Miya| 30|drink|\n",
      "|  3|Nana| 26|berry|\n",
      "+---+----+---+-----+"
     ]
    }
   ],
   "source": [
    "# RDD創建表 - createDataFrame\n",
    "\n",
    "def create_df_from_rdd():\n",
    "    spark_rdd = spark.sparkContext.parallelize([\n",
    "        (1, \"Suny\", 10, \"water\"),\n",
    "        (2, \"Miya\", 30, \"drink\"),\n",
    "        (3, \"Nana\", 26, \"berry\")])\n",
    " \n",
    "    # 設置屬性\n",
    "    schema = StructType([StructField(\"id\", LongType(), True),\n",
    "                         StructField(\"name\", StringType(), True),\n",
    "                         StructField(\"age\", LongType(), True),\n",
    "                         StructField(\"text\", StringType(), True)])\n",
    "    \n",
    "    # 建立DataFrame\n",
    "    df_from_rdd = spark.createDataFrame(spark_rdd, schema)\n",
    "    \n",
    "    # 建立臨時表\n",
    "    df_from_rdd.createOrReplaceTempView(\"myTempView\")\n",
    "    # 使用SQL\n",
    "    data = spark.sql(\"select * from myTempView\")\n",
    "    data.show()\n",
    "    \n",
    "create_df_from_rdd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+-----+\n",
      "| id|name|age| text|\n",
      "+---+----+---+-----+\n",
      "|  1|Suny| 10|water|\n",
      "|  2|Miya| 30|drink|\n",
      "|  3|Nana| 26|berry|\n",
      "+---+----+---+-----+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- text: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "# RDD創建表 - toDF\n",
    "\n",
    "def create_df_from_rdd():\n",
    "    spark_rdd = spark.sparkContext.parallelize([\n",
    "        (1, \"Suny\", 10, \"water\"),\n",
    "        (2, \"Miya\", 30, \"drink\"),\n",
    "        (3, \"Nana\", 26, \"berry\")])\n",
    " \n",
    "    # 設置屬性\n",
    "    schema = StructType([StructField(\"id\", LongType(), True),\n",
    "                         StructField(\"name\", StringType(), True),\n",
    "                         StructField(\"age\", LongType(), True),\n",
    "                         StructField(\"text\", StringType(), True)])\n",
    "    \n",
    "    # toDF 建立\n",
    "    df_from_rdd = spark_rdd.toDF(schema)\n",
    "    \n",
    "    # 建立臨時表\n",
    "    df_from_rdd.createOrReplaceTempView(\"myTempView\")\n",
    "    # 使用SQL\n",
    "    data = spark.sql(\"select * from myTempView\")\n",
    "    return data\n",
    "    \n",
    "create_df_from_rdd().show()\n",
    "create_df_from_rdd().printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+-----+\n",
      "| id|name|age| text|\n",
      "+---+----+---+-----+\n",
      "|  1|Miya| 10|water|\n",
      "|  2|Nana| 30|drink|\n",
      "|  3|Nana| 26|berry|\n",
      "+---+----+---+-----+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- text: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "# RDD創建表 - Row\n",
    "\n",
    "def create_df_from_rdd():\n",
    "    \n",
    "    row =Row(\"id\",\"name\",\"age\",\"text\")\n",
    "    a = [1,2,3,3]\n",
    "    b = [\"Miya\", \"Nana\", \"Nana\"]\n",
    "    c = [10,30,26]\n",
    "    d = [\"water\", \"drink\", \"berry\"]\n",
    "    spark_rdd = spark.sparkContext.parallelize([row(a[i],b[i],c[i],d[i]) for i in range(3)])\n",
    " \n",
    "    # 設置屬性\n",
    "    schema = StructType([StructField(\"id\", StringType(), True),\n",
    "                         StructField(\"name\", StringType(), True),\n",
    "                         StructField(\"age\", StringType(), True),\n",
    "                         StructField(\"text\", StringType(), True)])\n",
    "    \n",
    "    # toDF 建立\n",
    "    df_from_rdd = spark_rdd.toDF(schema)\n",
    "    \n",
    "    # 建立臨時表\n",
    "    df_from_rdd.createOrReplaceTempView(\"myTempView\")\n",
    "    # 使用SQL\n",
    "    data = spark.sql(\"select * from myTempView\")\n",
    "    return data\n",
    "\n",
    "create_df_from_rdd().show()\n",
    "create_df_from_rdd().printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+-----+----+-----+\n",
      "|lsoa_code|             borough|      major_category|      minor_category|value|year|month|\n",
      "+---------+--------------------+--------------------+--------------------+-----+----+-----+\n",
      "|E01004563|          Wandsworth|             Robbery|   Personal Property|    0|2008|    6|\n",
      "|E01003496|              Newham|     Criminal Damage|Criminal Damage T...|    0|2013|    9|\n",
      "|E01000086|Barking and Dagenham|  Theft and Handling|  Other Theft Person|    1|2009|    5|\n",
      "|E01001317|              Ealing|            Burglary|Burglary in a Dwe...|    0|2013|    9|\n",
      "|E01000713|             Bromley|Violence Against ...|    Offensive Weapon|    0|2009|   12|\n",
      "+---------+--------------------+--------------------+--------------------+-----+----+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# csv創建表\n",
    "\n",
    "def create_df_from_csv():\n",
    "    df_from_csv = spark.read.csv('hdfs://10.120.26.200/user/spark/spark_sql_101/crime/data/', header=True, inferSchema=True)\n",
    "    return df_from_csv\n",
    "\n",
    "create_df_from_csv().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-------------------+------------------+\n",
      "|                  a|                 b|                  c|                 d|\n",
      "+-------------------+------------------+-------------------+------------------+\n",
      "| 0.8205575047432271|0.5478281615029911| 0.8817539078301593|0.6536857088691732|\n",
      "| 0.6370703218878147|0.6398977672193277|  0.549795106315854|0.8065077341224987|\n",
      "|0.25685015746712214|0.5316804678523885|0.46278865500266375|0.4203465970220044|\n",
      "+-------------------+------------------+-------------------+------------------+"
     ]
    }
   ],
   "source": [
    "# pandas創建表\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_df_from_pandas():\n",
    "    df = pd.DataFrame(np.random.random((3, 4)))\n",
    "    df_from_pandas = spark.createDataFrame(df, schema=['a', 'b', 'c', 'd'])\n",
    "    return df_from_pandas\n",
    "    \n",
    "create_df_from_pandas().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
